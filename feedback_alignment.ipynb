{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wll7qhCUGcyh"
      },
      "source": [
        "\n",
        "\n",
        "### Importing Libraries and Load Data\n",
        "\n",
        "In this cell, we import the necessary libraries and load the MNIST dataset.\n",
        "\n",
        "- **Libraries Imported**:\n",
        "  - `numpy`: For numerical operations.\n",
        "  - `torch` and `torch.nn`: For building and training neural networks.\n",
        "  - `torch.optim`: For optimization algorithms.\n",
        "  - `torchvision` and `torchvision.transforms`: For handling and transforming datasets.\n",
        "  - `matplotlib.pyplot`: For plotting graphs.\n",
        "  - `random`: For generating random numbers.\n",
        "  - `sklearn.metrics`: For calculating evaluation metrics like classification report, confusion matrix, and F1 score.\n",
        "\n",
        "- **Data Loading**:\n",
        "  - We define a transformation for the dataset which includes converting images to PyTorch tensors and normalizing them to the range [-1, 1].\n",
        "  - We load the MNIST dataset using `torchvision.datasets.MNIST`. The training data is loaded into `trainloader` and the test data into `testloader`. We set a batch size of 128 and shuffle the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCFZjiCdGgQW",
        "outputId": "be8e3dde-991b-44d2-c63c-277833890bcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:02<00:00, 4615568.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 134324.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:06<00:00, 245593.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 2871650.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "\n",
        "# Define the transformation for the dataset (normalize the images)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize the images to [-1, 1] range\n",
        "])\n",
        "\n",
        "# Load the MNIST dataset for training and testing\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npikltdgGhWT"
      },
      "source": [
        "### Define the Model Architecture\n",
        "\n",
        "In this cell, we define the architecture of our neural network.\n",
        "\n",
        "- **Model Architecture**:\n",
        "  - **Input Layer**: The input layer takes the 28x28 images (flattened to 784).\n",
        "  - **Hidden Layer 1**: A fully connected layer with 512 units followed by a ReLU activation function and a dropout layer with a 0.2 dropout rate.\n",
        "  - **Hidden Layer 2**: Another fully connected layer with 512 units followed by a ReLU activation function and a dropout layer with a 0.2 dropout rate.\n",
        "  - **Output Layer**: A fully connected layer with 10 units (one for each class) followed by a softmax activation function to produce probabilities for multi-class classification.\n",
        "\n",
        "- **PyTorch Implementation**:\n",
        "  - We create a class `SimpleNN` that inherits from `nn.Module`.\n",
        "  - The `__init__` method initializes the layers.\n",
        "  - The `forward` method defines the forward pass of the network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IeqCgEuHMJG"
      },
      "outputs": [],
      "source": [
        "# Define the model architecture\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 512)  # First fully connected layer\n",
        "        self.fc2 = nn.Linear(512, 512)  # Second fully connected layer\n",
        "        self.fc3 = nn.Linear(512, 10)  # Output layer\n",
        "        self.dropout = nn.Dropout(0.2)  # Dropout layer to prevent overfitting\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)  # Flatten the input image\n",
        "        x = torch.relu(self.fc1(x))  # Apply ReLU activation function\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = torch.relu(self.fc2(x))  # Apply ReLU activation function\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = self.fc3(x)  # Output layer\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3asWkaBIGUh"
      },
      "source": [
        "### Evaluation Function\n",
        "\n",
        "In this cell, we define a function to evaluate the model's accuracy.\n",
        "\n",
        "- **Function Details**:\n",
        "  - The `evaluate` function sets the model to evaluation mode using `model.eval()`.\n",
        "  - We disable gradient computation with `torch.no_grad()` to save memory and computations during evaluation.\n",
        "  - We loop over the test dataset, pass the images through the model, and get the predicted class with the highest probability.\n",
        "  - We count the number of correct predictions and calculate the accuracy as the ratio of correct predictions to the total number of images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FDlQs56HRdJ"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate model accuracy\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
        "        for data in dataloader:\n",
        "            images, labels = data\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)  # Get the index of the max log-probability\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
        "    return correct / total  # Return accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxZjAkO7J8TD"
      },
      "source": [
        "### Training Function for Weight Perturbation\n",
        "\n",
        "In this cell, we implement the training function for the weight perturbation learning rule.\n",
        "\n",
        "- **Function Details**:\n",
        "  - The `train_weight_perturbation` function takes the model, training data loader, learning rate, and number of epochs as inputs.\n",
        "  - We use `CrossEntropyLoss` as the loss function and `Adam` as the optimizer.\n",
        "  - For each epoch, we iterate over the training data, zero the parameter gradients, and perform a forward pass to compute the outputs and the loss.\n",
        "  - We backpropagate the loss to compute gradients.\n",
        "  - We then perturb the weights by adding and subtracting random noise scaled by the learning rate.\n",
        "  - Finally, we update the model parameters using the optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRa13NfbJ-ek"
      },
      "outputs": [],
      "source": [
        "# Training function for weight perturbation\n",
        "def train_weight_perturbation(model, trainloader, learning_rate, epochs):\n",
        "    criterion = nn.CrossEntropyLoss()  # Define the loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Define the optimizer\n",
        "    for epoch in range(epochs):\n",
        "        for data in trainloader:\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()  # Zero the parameter gradients\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)  # Compute the loss\n",
        "            loss.backward()  # Backpropagate the loss\n",
        "\n",
        "            # Perturb weights\n",
        "            with torch.no_grad():\n",
        "                for param in model.parameters():\n",
        "                    param += torch.randn_like(param) * learning_rate  # Add random noise\n",
        "                    param -= torch.randn_like(param) * learning_rate  # Subtract random noise\n",
        "\n",
        "            optimizer.step()  # Update parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDmHRDHQJ_Uj"
      },
      "source": [
        "### Training Function for Feedback Alignment\n",
        "\n",
        "In this cell, we implement the training function for the feedback alignment learning rule.\n",
        "\n",
        "- **Function Details**:\n",
        "  - The `train_feedback_alignment` function takes the model, training data loader, learning rate, and number of epochs as inputs.\n",
        "  - We use `CrossEntropyLoss` as the loss function and `Adam` as the optimizer.\n",
        "  - We initialize random feedback weights for each parameter of the model.\n",
        "  - For each epoch, we iterate over the training data, zero the parameter gradients, and perform a forward pass to compute the outputs and the loss.\n",
        "  - We backpropagate the loss to compute gradients.\n",
        "  - We adjust the gradients using the feedback weights.\n",
        "  - Finally, we update the model parameters using the optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q57ws_xpJ_q8"
      },
      "outputs": [],
      "source": [
        "# Training function for feedback alignment\n",
        "def train_feedback_alignment(model, trainloader, learning_rate, epochs):\n",
        "    criterion = nn.CrossEntropyLoss()  # Define the loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Define the optimizer\n",
        "    feedback_weights = [torch.randn_like(p) for p in model.parameters()]  # Initialize random feedback weights\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for data in trainloader:\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()  # Zero the parameter gradients\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)  # Compute the loss\n",
        "            loss.backward()  # Backpropagate the loss\n",
        "\n",
        "            # Apply feedback alignment\n",
        "            with torch.no_grad():\n",
        "                for p, f in zip(model.parameters(), feedback_weights):\n",
        "                    p.grad -= f  # Adjust the gradient using the feedback weights\n",
        "                optimizer.step()  # Update parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iouw5AJzKB9k"
      },
      "source": [
        "### Measure Performance Metrics\n",
        "\n",
        "In this cell, we measure various performance metrics for the model trained with different learning rules.\n",
        "\n",
        "- **Function Details**:\n",
        "  - The `measure_performance` function takes the model, training and test data loaders, learning rate, number of epochs, and the learning rule (algorithm) as inputs.\n",
        "  - We select the appropriate training function based on the learning rule (either weight perturbation or feedback alignment).\n",
        "  - **Convergence Speed**: We measure the epoch at which the model reaches a predefined performance threshold (e.g., 90% accuracy).\n",
        "  - **Final Accuracy**: We record the final test accuracy after training.\n",
        "  - **Sensitivity to Learning Rate**: We train the model with various learning rates and plot the accuracy for each learning rate to understand how the performance varies.\n",
        "  - **Robustness Against Noise**: We add different levels of noise to the test data and measure the model's accuracy to evaluate its robustness.\n",
        "  - We plot graphs to visualize the sensitivity to learning rate and robustness against noise.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ksfrj3PKCS6"
      },
      "outputs": [],
      "source": [
        "# Measure performance metrics\n",
        "def measure_performance(model, trainloader, testloader, learning_rate, epochs, algo='backprop'):\n",
        "    # Select the appropriate training function\n",
        "    if algo == 'weight_perturbation':\n",
        "        train_func = train_weight_perturbation\n",
        "    elif algo == 'feedback_alignment':\n",
        "        train_func = train_feedback_alignment\n",
        "    else:\n",
        "        raise ValueError(\"Unknown algorithm\")\n",
        "\n",
        "    # Measure convergence speed\n",
        "    performance_threshold = 0.90\n",
        "    convergence_epoch = None\n",
        "    accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_func(model, trainloader, learning_rate, 1)\n",
        "        accuracy = evaluate(model, testloader)\n",
        "        accuracies.append(accuracy)\n",
        "        if accuracy >= performance_threshold and convergence_epoch is None:\n",
        "            convergence_epoch = epoch\n",
        "\n",
        "    final_accuracy = accuracies[-1]\n",
        "    print(f'Final Test Accuracy: {final_accuracy}')\n",
        "    print(f'Convergence Epoch: {convergence_epoch}')\n",
        "\n",
        "    # Measure sensitivity to learning rate\n",
        "    learning_rates = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
        "    lr_accuracies = []\n",
        "    for lr in learning_rates:\n",
        "        model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
        "        train_func(model, trainloader, lr, epochs)\n",
        "        lr_accuracies.append(evaluate(model, testloader))\n",
        "\n",
        "    plt.plot(learning_rates, lr_accuracies)\n",
        "    plt.xscale('log')\n",
        "    plt.xlabel('Learning Rate')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Sensitivity to Learning Rate')\n",
        "    plt.show()\n",
        "\n",
        "    # Measure robustness against noise\n",
        "    noise_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "    noise_accuracies = []\n",
        "    for noise in noise_levels:\n",
        "        noisy_testset = testset\n",
        "        noisy_testset.data = testset.data + torch.randn_like(testset.data) * noise  # Add noise to the data\n",
        "        noisy_testloader = torch.utils.data.DataLoader(noisy_testset, batch_size=128, shuffle=False)\n",
        "        noise_accuracies.append(evaluate(model, noisy_testloader))\n",
        "\n",
        "    plt.plot(noise_levels, noise_accuracies)\n",
        "    plt.xlabel('Noise Level')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Robustness Against Noise')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru2hMSCLKFPN"
      },
      "source": [
        "### Main Function to Run All Tests\n",
        "\n",
        "In this cell, we define the main function to run the tests for both weight perturbation and feedback alignment learning rules.\n",
        "\n",
        "- **Function Details**:\n",
        "  - The `main` function initializes a `SimpleNN` model.\n",
        "  - We set a learning rate of 0.001 and the number of epochs to 10.\n",
        "  - We first test the weight perturbation learning rule by calling `measure_performance` with the appropriate arguments.\n",
        "  - We reinitialize the model and then test the feedback alignment learning rule by calling `measure_performance` again.\n",
        "  - The results for both learning rules are printed and visualized.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "3Poe_ni8KFgp",
        "outputId": "9a5a8663-a24b-4b64-ec4b-4e63043c1392"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Weight Perturbation\n",
            "Final Test Accuracy: 0.9781\n",
            "Convergence Epoch: 0\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4cdaf36c1e1d>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Run the main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-4cdaf36c1e1d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testing Weight Perturbation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmeasure_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weight_perturbation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reinitialize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-e577f056f842>\u001b[0m in \u001b[0;36mmeasure_performance\u001b[0;34m(model, trainloader, testloader, learning_rate, epochs, algo)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlearning_rates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reset_parameters'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mtrain_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mlr_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-a6376c929159>\u001b[0m in \u001b[0;36mtrain_weight_perturbation\u001b[0;34m(model, trainloader, learning_rate, epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0mper_device_and_dtype_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zero_grad_profile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/profiler.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisableTorchFunctionSubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RecordFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m         )\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m         \u001b[0;31m# use `self_` to avoid naming collide with aten ops arguments that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0;31m# are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Main function to run all tests\n",
        "def main():\n",
        "    model = SimpleNN()  # Initialize the model\n",
        "    learning_rate = 1e-3  # Set the learning rate\n",
        "    epochs = 10  # Set the number of epochs\n",
        "\n",
        "    print(\"Testing Weight Perturbation\")\n",
        "    measure_performance(model, trainloader, testloader, learning_rate, epochs, algo='weight_perturbation')\n",
        "\n",
        "    model = SimpleNN()  # Reinitialize the model\n",
        "    print(\"Testing Feedback Alignment\")\n",
        "    measure_performance(model, trainloader, testloader, learning_rate, epochs, algo='feedback_alignment')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()  # Run the main function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLtwXsv5kQwo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
