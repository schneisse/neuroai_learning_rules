{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLygbz278oF1"
      },
      "source": [
        "# Supervised learning using predictive coding\n",
        "\n",
        "This notebook illustrates how the predictive coding library can be used to train a predictive coding network on a supervised learning task (MNIST)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJO9KSFm8oF4",
        "outputId": "a16b6ec8-b53d-4dab-88ba-f3510c43d014"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "import Predictive as pc\n",
        "\n",
        "torch.manual_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'using {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g72NMl0x8oF5",
        "outputId": "94fbdffc-644c-4dd2-f020-1e7a181226db"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: torch.flatten(x))])\n",
        "train_dataset = datasets.MNIST('./data', download=True, train=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./data', download=True, train=False, transform=transform)\n",
        "\n",
        "batch_size = 500\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(f'# train images: {len(train_dataset)} and # test images: {len(test_dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZveDwPLX8oF5"
      },
      "source": [
        "### Defining a model\n",
        "A model can be initalised in the same way as pytorch model, with the addition of `pc.PCLayer()` to include latent variables in the model.\n",
        "\n",
        "A `PCLayer()` contains the activities of a layer of latent variables under `pclayer._x`. A `PCLayer()` also contains the energy associated with that activity under `pclayer._energy` which is computed with `0.5 *(inputs['mu'] - inputs['x'])**2` where `inputs['x']` is the activity of that layer and `inputs['mu']` is the input to that layer.\n",
        "\n",
        "Check out the `PCLayer()` class in `predictive_coding/pc_layer.py` for more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbcZEBnH8oF5",
        "outputId": "7fdc4413-3532-434d-b2eb-3e7ca41d727b"
      },
      "outputs": [],
      "source": [
        "input_size = 28*28  # 28x28 images\n",
        "hidden_size = 256\n",
        "output_size = 10    # 10 classes\n",
        "activation_fn = torch.nn.ReLU\n",
        "loss_fn = lambda output, _target: 0.5 * (output - _target).pow(2).sum() # this loss function holds to the error of the output layer of the model\n",
        "\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(input_size, hidden_size),\n",
        "    pc.PredictiveLayer(),\n",
        "    activation_fn(),\n",
        "    nn.Linear(hidden_size, hidden_size),\n",
        "    pc.PredictiveLayer(),\n",
        "    activation_fn(),\n",
        "    nn.Linear(hidden_size, output_size)\n",
        ")\n",
        "model.train()   # set the model to training mode\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGR08KhU8oF5"
      },
      "source": [
        "### Defining a model trainer\n",
        "The predictive coding library is based around a `pc.PCTrainer()`.\n",
        "\n",
        "This trainer orchestrate the activity and parameter updates of the model to minimise the total error of the model. The total error is given by the sum of the energies in each pclayer as well as the loss functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrTV_m728oF6"
      },
      "outputs": [],
      "source": [
        "# number of inference iterations where the latent states x are updated. Inference does not run till convergence but for a fixed number of iterations\n",
        "T = 20\n",
        "\n",
        "# options for the update of the latent state x\n",
        "optimizer_x_fn = optim.SGD          # optimizer for latent state x, SGD perform gradient descent. Other alternative are Adam, RMSprop, etc.\n",
        "\n",
        "# options for the update of the parameters p\n",
        "optimizer_p_fn = optim.Adam         # optimizer for parameters p\n",
        "\n",
        "trainer = pc.PredictiveTrainer(model=model,\n",
        "    T = T,\n",
        "    optimizer_x_fn = optimizer_x_fn,\n",
        "    optimizer_p_fn = optimizer_p_fn,\n",
        "    x_lr = 0.01,\n",
        "    p_lr = 0.001,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thtspenN8oF6"
      },
      "outputs": [],
      "source": [
        "# get classification accuracy of the model\n",
        "def test(model, dataset, batch_size=1000):\n",
        "    model.eval()\n",
        "    test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data, label in test_loader:\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        pred = model(data)\n",
        "        _, predicted = torch.max(pred, -1)\n",
        "        total += label.size(0)\n",
        "        correct += (predicted == label).sum().item()\n",
        "    model.train()\n",
        "    return round(correct / total, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7c9AIGY8oF6"
      },
      "source": [
        "### Train the model\n",
        "`trainer.train_on_batch()` is called for each batch of data. This function updates the activity of the latent states and the parameters for the given batch of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "\n",
        "test_acc = np.zeros(epochs + 1)\n",
        "test_acc[0] = test(model, test_dataset)\n",
        "for epoch in range(epochs):\n",
        "    # Initialize the tqdm progress bar\n",
        "    with tqdm(train_loader, desc=f'Epoch {epoch+1} - Test accuracy: {test_acc[epoch]:.3f}') as pbar:\n",
        "        for data, label in pbar:\n",
        "            data, label = data.to(device), label.to(device)\n",
        "            # convert labels to one-hot encoding\n",
        "            label = F.one_hot(label, num_classes=output_size).float()\n",
        "            trainer.train_batch(\n",
        "                inputs=data,\n",
        "                loss_fn=loss_fn,\n",
        "                label=label\n",
        "            )\n",
        "    test_acc[epoch + 1] = test(model, test_dataset)\n",
        "    pbar.set_description(f'Epoch {epoch + 1} - Test accuracy: {test_acc[epoch + 1]:.3f}')\n",
        "\n",
        "plt.plot(test_acc)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIlwOcPP8oF6"
      },
      "source": [
        "The trained model achieves a classification accuracy of above 95% on MNIST which is comparable to a backpropagation trained model with the same architecture."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
